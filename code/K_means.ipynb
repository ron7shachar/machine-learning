{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Uploud the data"
      ],
      "metadata": {
        "id": "fhNXI6uVUc-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yE7Zkis6B4v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from random import*\n",
        "from google.colab import drive\n",
        "from scipy.stats import multivariate_normal\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold,RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# Step 2: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 4: Read the CSV file into a DataFrame\n",
        "Xy_train = pd.read_csv(\"/content/drive/Shareddrives/ML/חלק ב/AirlineSatisfaction Dataset/Xy_train.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing"
      ],
      "metadata": {
        "id": "hWsuZrF5jsN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN3SJ_Burn4H"
      },
      "outputs": [],
      "source": [
        "train_data = Xy_train\n",
        "\n",
        "# Remove unwanted class\n",
        "train_data[\"Class\"][train_data[\"Class\"] == \"IT IS SO BORING WORKING IN AN AIRPORT'S DESK OH MY GODDDDD\"] = None\n",
        "train_data[\"Class\"][train_data[\"Class\"] == 'Unknown'] = None\n",
        "\n",
        "for column in train_data[:-1].columns:train_data[column] = train_data[column].fillna(np.random.choice(train_data[column][~train_data[column].isna()]))\n",
        "\n",
        "to_plot = train_data[[\"Type of Travel\",\"Inflight wifi service\" , \"Cleanliness\",\"Customer Type\",\"Inflight service\",'Flight Distance','satisfaction']].copy(deep=True)\n",
        "\n",
        "categorical_columns = train_data.select_dtypes(include=['object', 'category']).columns\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column])\n",
        "    label_encoders[column] = le\n",
        "cat = []\n",
        "for column in train_data.columns:\n",
        "  if len(train_data[column].unique()) <=30 and column != 'satisfaction':\n",
        "    cat.append(column)\n",
        "train_data = pd.get_dummies(train_data, columns=cat)\n",
        "\n",
        "print('______________________________________________________________________________________________________________________-')\n",
        "for column in train_data.columns: print(f\"column : {column} -> {list(train_data[column].unique())}\")\n",
        "\n",
        "y_train = train_data['satisfaction']\n",
        "X_train = train_data.drop(columns=['satisfaction'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "mYEV7kOVyN0l"
      },
      "source": [
        "####  look at the data to see if there are seen clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f648oGQCyN0l"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(to_plot, hue='satisfaction')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1p517VGeyN0m"
      },
      "source": [
        " Since this data is four dimensional, we can reduce it to two which allows to visualize the data with low reduction in explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4X0n7CyQyN0m"
      },
      "outputs": [],
      "source": [
        "\n",
        "pca = PCA(n_components=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UBEfm5vwyN0m"
      },
      "outputs": [],
      "source": [
        "pca.fit(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FrH2Go18yN0m"
      },
      "outputs": [],
      "source": [
        "train_data_pca = pca.transform(X_train)\n",
        "train_data_pca = pd.DataFrame(train_data_pca, columns=['PC1', 'PC2'])\n",
        "train_data_pca['satisfaction'] = y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sd_FMFhKSV6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DX6WnwR0yN0n"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x='PC1', y='PC2', hue='satisfaction', data=train_data_pca)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ShKHN4CZyN0n"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, max_iter=300, n_init=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "id": "9KpUansdyN0n"
      },
      "outputs": [],
      "source": [
        "kmeans.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "1oceJYHikYGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9SlB-O4kyN0n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 4. הרצת K-Means עם 2 אשכולות\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "cluster_labels = kmeans.labels_\n",
        "k_predict = kmeans.predict(X_train)\n",
        "\n",
        "ari_score = adjusted_rand_score(y_train.apply(lambda x :not x), cluster_labels)\n",
        "print(f'Adjusted Rand Index (ARI): {ari_score}')\n",
        "f1 = f1_score(y_train.apply(lambda x :not x), k_predict)\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6BcBKL5-yN0n"
      },
      "outputs": [],
      "source": [
        "train_data_pca['satisfaction'] = k_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7c2trFk-yN0o"
      },
      "source": [
        "#### present the points with the cluster centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HZkQzF65yN0o"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x='PC1', y='PC2', hue='satisfaction', data=train_data_pca, palette='Accent')\n",
        "plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0], pca.transform(kmeans.cluster_centers_)[:, 1], marker='+', s=100 ,color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "t_PU_uO3yN0o"
      },
      "source": [
        "#### Now, what can we do if we dont know the number of clusters ?\n",
        "#### what are the main two clusters properies we want?\n",
        "Homogeneity, seperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "T2tH1eWSyN0p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tqdm import tqdm\n",
        "iner_list = []\n",
        "dbi_list = []\n",
        "sil_list = []\n",
        "\n",
        "for n_clusters in tqdm(range(2, 10, 1)):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, max_iter=300, n_init=10, random_state=42)\n",
        "    kmeans.fit(X_train)\n",
        "    assignment = kmeans.predict(X_train)\n",
        "\n",
        "    iner = kmeans.inertia_\n",
        "    sil = silhouette_score(X_train, assignment)\n",
        "    dbi = davies_bouldin_score(X_train, assignment)\n",
        "\n",
        "    dbi_list.append(dbi)\n",
        "    sil_list.append(sil)\n",
        "    iner_list.append(iner)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot measure"
      ],
      "metadata": {
        "id": "2ihSpPweUQo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ElJztca8yN0p"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(2, 10, 1), iner_list, marker='o')\n",
        "plt.title(\"Inertia\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(2, 10, 1), sil_list, marker='o')\n",
        "plt.title(\"Silhouette\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(2, 10, 1), dbi_list, marker='o')\n",
        "plt.title(\"Davies-Bouldin\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cOpUoCxqyN0p"
      },
      "source": [
        "Inertia is a measure of homogenity - Sum of squared distances of samples to their closest cluster center.\n",
        "\n",
        "Davies bouldin score and Silhouette measure mixed homogenity and separation.\n",
        "\n",
        "Better clustring represents by lower inertia and Davies-Bouldin measures with higher Silhouette score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, max_iter=300, n_init=10, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "cluster_labels = kmeans.labels_\n",
        "train_data_pca['satisfaction'] = kmeans.predict(X_train)\n",
        "\n",
        "# ari_score = adjusted_rand_score(y_train.apply(lambda x :not x), cluster_labels)\n",
        "# print(f'Adjusted Rand Index (ARI): {ari_score}')\n",
        "# f1 = f1_score(y_train.apply(lambda x :not x), k_predict)\n",
        "# print(f'F1 Score: {f1}')\n",
        "\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='satisfaction', data=train_data_pca, palette='Accent')\n",
        "plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0], pca.transform(kmeans.cluster_centers_)[:, 1], marker='+', s=100 ,color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNBFpdizkySk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}